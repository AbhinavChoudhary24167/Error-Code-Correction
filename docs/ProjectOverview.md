# Project Overview

This document expands on the README by describing how the pieces of the
Error-Code-Correction repository interact.  It is intended for researchers and
engineers who want to understand the motivation behind the codebase before
running simulations or extending the framework.

## High-Level Goals

The toolkit quantifies how different error-correcting-code (ECC) schemes impact
SRAM-based memories.  It answers questions such as:

- How many soft errors will a given workload experience at a particular process
  node and supply voltage?
- Which ECC variant (SEC-DED, SEC-DAEC, BCH, TAEC) provides sufficient
  protection without overspending energy or area?
- What is the sustainability cost of the additional corrections and background
  scrubbing required to keep the memory reliable?

Achieving these goals requires both bit-accurate simulations and high-level
analytics, which is why the repository mixes C++ binaries, Python packages and
structured data files.

## Components at a Glance

- **`Hamming32bit1Gb.cpp`, `Hamming64bit128Gb.cpp`** – C++ simulators that inject
  faults into sparse memory models and report how well the selected ECC handles
  them.
- **`BCHvsHamming.cpp`** – Side-by-side evaluator that compares BCH(63,51,2) with
  the in-house Hamming implementation across a battery of fault patterns.
- **`SAT.cpp`** – Minimal DPLL SAT solver used to test logical properties of the
  ECC formulations.
- **`src/`, `telemetry.hpp`, `gate_energy.hpp`** – Shared code for loading energy
  models, describing faults and writing structured logs.
- **`configs/`, `data/`, `tech_calib.json`** – Parameter files that define
  technology operating points, workload assumptions and calibrated energy
  numbers.
- **`eccsim.py`, `analysis/`, `plot_pareto.py`** – Python utilities that turn raw
  simulator logs into Pareto frontiers, trade-off reports and visualisations.
- **`reports/`** – Example outputs generated by helper scripts; useful as
  reference when validating new results.

## Typical Workflow

1. **Select a technology point** using the calibration and configuration files.
   These describe retention time, multi-bit upset (MBU) rates and scrub
   intervals derived from silicon data or literature surveys.
2. **Run the C++ simulators** to evaluate how a candidate ECC behaves under the
   chosen fault model.  Each simulator writes human-readable summaries and CSV /
   JSON logs such as `ecc_stats.*` and `decoding_results.*`.
3. **Feed the logs into the Python analysis layer**.  Tools like `eccsim
   reliability-report` or `analysis/pareto.py` aggregate the raw counts into
   reliability metrics (FIT/MTTF), energy consumption and carbon footprint.
4. **Compare alternatives** by generating Pareto frontiers (`pareto.csv`),
   computing trade-offs (`tradeoffs.json`) and labelling design archetypes
   (`archetype.json`).  These artefacts guide decisions about which ECC to ship.

The workflow is modular: simulations can be swapped out, new calibration files
can be added and external datasets can be ingested via `parse_telemetry.py`.

## Data and Sustainability Modelling

The repository emphasises the link between reliability improvements and
sustainability impacts.  Calibration files include per-technology energy
consumption, while tools such as `carbon.py` and `energy_model.py` estimate how
extra scrubbing and decoding affect operational carbon.  Generated reports use
fields like `E_scrub_kWh` and `carbon_kg` to quantify those impacts explicitly.

## Extending the Framework

When adding a new ECC scheme or adapting the project to a different process
node:

1. Add or modify calibration data under `configs/` and `data/` with the relevant
   fault statistics and energy numbers.
2. Implement the ECC logic in C++ (or extend an existing simulator) and ensure
   the output format matches the existing CSV / JSON conventions.
3. Update the Python analysis scripts to recognise the new scheme so that it can
   participate in Pareto and trade-off reports.
4. Document the addition in the README and in `docs/` so future collaborators
   can reproduce the evaluation.

Following these steps keeps the toolkit consistent while enabling new research
questions to be explored quickly.
